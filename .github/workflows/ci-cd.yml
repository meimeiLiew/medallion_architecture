name: Medallion Pipeline CI/CD

on:
  push:
    branches: [ main, develop ]
  pull_request:
    branches: [ main, develop ]
  workflow_dispatch:

jobs:
  test:
    name: Test
    runs-on: ubuntu-latest
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        pip install pytest pytest-cov flake8
        
    - name: Lint with flake8
      run: |
        flake8 . --count --select=E9,F63,F7,F82 --show-source --statistics
        
    - name: Run unit tests
      run: |
        pytest tests/unit_tests/ --cov=./ --cov-report=xml
        
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        fail_ci_if_error: true
        
  validate-dbt:
    name: Validate dbt Models
    runs-on: ubuntu-latest
    needs: test
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install dbt-core dbt-bigquery
        
    - name: Validate dbt project
      run: |
        cd silver/transformations
        dbt --version
        dbt parse --profiles-dir=./profiles
        
    - name: Run dbt tests (compile only)
      run: |
        cd silver/transformations
        dbt test --profiles-dir=./profiles --compile
        
  validate-airflow:
    name: Validate Airflow DAGs
    runs-on: ubuntu-latest
    needs: test
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.10'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install apache-airflow==2.5.0 apache-airflow-providers-google
        
    - name: Validate DAGs
      run: |
        python -c "
        import os
        import sys
        from airflow.models import DagBag
        
        # Set Airflow home to the current directory
        os.environ['AIRFLOW_HOME'] = os.getcwd()
        
        # Load DAGs
        dag_bag = DagBag(dag_folder='orchestration/airflow/dags', include_examples=False)
        
        # Check for import errors
        if dag_bag.import_errors:
            print('DAG import errors:')
            for file, error in dag_bag.import_errors.items():
                print(f'{file}: {error}')
            sys.exit(1)
            
        # Check DAG structure
        for dag_id, dag in dag_bag.dags.items():
            print(f'Validating DAG: {dag_id}')
            # Validate DAG structure
            dag.test_cycle()
        
        print('All DAGs validated successfully!')
        "
        
  deploy-dev:
    name: Deploy to Development
    runs-on: ubuntu-latest
    needs: [validate-dbt, validate-airflow]
    if: github.ref == 'refs/heads/develop'
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Google Cloud SDK
      uses: google-github-actions/setup-gcloud@v1
      with:
        project_id: ${{ secrets.GCP_PROJECT_ID }}
        service_account_key: ${{ secrets.GCP_SA_KEY }}
        export_default_credentials: true
        
    - name: Deploy dbt models to development
      run: |
        echo "Deploying dbt models to development environment..."
        # In a real scenario, you would use dbt Cloud API or gcloud commands to deploy
        # Example: curl -X POST "https://cloud.getdbt.com/api/v2/accounts/{account_id}/jobs/{job_id}/run/" -H "Authorization: Token ${{ secrets.DBT_CLOUD_TOKEN }}"
        
    - name: Deploy Airflow DAGs to development
      run: |
        echo "Deploying Airflow DAGs to development environment..."
        # In a real scenario, you would use gcloud commands to copy DAGs to the Airflow bucket
        # Example: gsutil cp -r orchestration/airflow/dags/* gs://${{ secrets.AIRFLOW_BUCKET }}/dags/
        
  deploy-prod:
    name: Deploy to Production
    runs-on: ubuntu-latest
    needs: [validate-dbt, validate-airflow]
    if: github.ref == 'refs/heads/main'
    
    steps:
    - uses: actions/checkout@v3
    
    - name: Set up Google Cloud SDK
      uses: google-github-actions/setup-gcloud@v1
      with:
        project_id: ${{ secrets.GCP_PROJECT_ID_PROD }}
        service_account_key: ${{ secrets.GCP_SA_KEY_PROD }}
        export_default_credentials: true
        
    - name: Deploy dbt models to production
      run: |
        echo "Deploying dbt models to production environment..."
        # In a real scenario, you would use dbt Cloud API or gcloud commands to deploy
        # Example: curl -X POST "https://cloud.getdbt.com/api/v2/accounts/{account_id}/jobs/{job_id}/run/" -H "Authorization: Token ${{ secrets.DBT_CLOUD_TOKEN }}"
        
    - name: Deploy Airflow DAGs to production
      run: |
        echo "Deploying Airflow DAGs to production environment..."
        # In a real scenario, you would use gcloud commands to copy DAGs to the Airflow bucket
        # Example: gsutil cp -r orchestration/airflow/dags/* gs://${{ secrets.AIRFLOW_BUCKET_PROD }}/dags/ 